{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  PREDICTION OF CUSTOMER CHURN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1.Business problem\n",
    "\n",
    "Customer churn is one of the major problem facing many companies due to it's direct impact on revenues. Telecommuinications field is one of those fields where customer churn is very rampant due to ever increasing competion.SyriaTel is one of the companies facing this challenge and is seeking to come up with predictive patterns and develop a robust classifier to forecast whether a customer is likely to churn in the near future. Our main goal is to develop a prediction model that will help SyriaTel predict customers who are likely to churn inorder to take necessary measures to reduce the churn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Data loading and Data Exploration\n",
    "We shall utilise Syria Tel dataset in seeking solution to our stated business problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'pandas' has no attribute 'core' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-23923692abc4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#importing relevant libraries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tester\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    178\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\testing.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \"\"\"\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m from pandas._testing import (\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0massert_extension_array_equal\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0massert_frame_equal\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\_testing.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m   2904\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2905\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2906\u001b[1;33m \u001b[0mcython_table\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSelectionMixin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cython_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2907\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2908\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'pandas' has no attribute 'core' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "#importing relevant libraries \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "import seaborn as sns \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score,cross_validate\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "from sklearn.metrics import roc_auc_score, ConfusionMatrixDisplay, classification_report, make_scorer,accuracy_score, confusion_matrix, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.model_selection import cross_val_score\n",
    "import multiprocessing # for reducing the runtime of gridsearch \n",
    "from sklearn.feature_selection import SelectFromModel \n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Reading from a CSV File\n",
    "df=pd.read_csv(\"syriatel.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset preview of the first 5 rows\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The shape of the data\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking datatypes\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the areas explored to prepare data for modeling.\n",
    "   1. Handling missing values\n",
    "   2. Duplicated rows\n",
    "   3. Identification and removal of Features that won't impact the analysis.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for duplicates \n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping irrelevant Features.\n",
    "Columns dropped :\n",
    "   1. State\n",
    "   2. Account length\n",
    "   3. Phone number\n",
    "\n",
    "\n",
    "Reasons For Dropping\n",
    "1. Account lenght- It's not applicable in this context since it doesn't give any information about customer's behaviour.In most cases these are random or sequential number assigned to new customers.\n",
    "2. State -\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping irrevant features\n",
    "irrelevant_features = ['account length','state','phone number']\n",
    "df.drop(columns = irrelevant_features, inplace =  True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separating categorical and continuous Features\n",
    "# Categorical columns\n",
    "categorical_columns = ['international plan', 'voice mail plan','churn']\n",
    "\n",
    "# Continuous columns\n",
    "continuous_columns = ['number vmail messages', 'total day minutes', 'total day calls',\n",
    "                'total day charge', 'total eve minutes', 'total eve calls',\n",
    "                'total eve charge', 'total night minutes', 'total night calls',\n",
    "                'total night charge', 'total intl minutes', 'total intl calls',\n",
    "                'total intl charge', 'customer service calls']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis will have 'churn' as the target/dependant variable.\n",
    "Churn has two classes :\n",
    "1. False : Customer has not terminated association with the telco.\n",
    "2. True  : Customer has  terminated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Depandant variable\n",
    "df.churn.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get a pie chart to analyze 'Churn' \n",
    "df['churn'].value_counts().plot.pie(explode=[0.1,0.1], autopct='%1.1f%%', startangle=90, shadow=True, figsize=(8,8))\n",
    "plt.title('Pie Chart for Churn')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pie chart shows imbalanced data distribution.\n",
    "14.5% of customers would actually churn,this can create a biased model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation Heatmap for Numeric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "correlation_matrix = df.corr()\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the characteristics exhibit no correlation, though a few demonstrate a complete positive correlation.\n",
    "Features with postive Correlation.\n",
    "total day charge \n",
    "total day minutes\n",
    "total eve charge\n",
    "total eve minutes\n",
    "total night charge\n",
    "total night minutes\n",
    "total int charge\n",
    "total int minutes\n",
    "\n",
    "This perfect correlation is logical as the charge directly corresponds to the minutes used.\n",
    "A correlation coefficientof 1 signifies perfect multicollinearity, which affects linear models \n",
    "differently from nonlinear models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformimg categorical data using OHE\n",
    "\n",
    "# Select the categorical columns to be one-hot encoded\n",
    "categorical_columns = ['international plan', 'voice mail plan']\n",
    "\n",
    "# Create an instance of the OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# Fit and transform the categorical columns\n",
    "encoded_categorical = encoder.fit_transform(df[categorical_columns])\n",
    "\n",
    "# Convert the encoded data to a DataFrame\n",
    "encoded_df = pd.DataFrame(encoded_categorical.toarray(), columns=encoder.get_feature_names_out(categorical_columns))\n",
    "\n",
    "# Concatenate the encoded DataFrame with the remaining columns from the original DataFrame\n",
    "df = pd.concat([df.drop(categorical_columns, axis=1), encoded_df], axis=1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tranfrom our dependant variable - 'churn' using LabelEncoder\n",
    "# Define the function to encode the column\n",
    "def encode(column):\n",
    "    Lencod = LabelEncoder()\n",
    "    df[column] = Lencod.fit_transform(df[column])\n",
    "\n",
    "# Call the function to encode the 'churn' column\n",
    "encode('churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the value counts for the encoded churn column\n",
    "print(df['churn'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Split into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into (X) and (y)\n",
    "y = df['churn']\n",
    "X = df.drop('churn',axis = 1)\n",
    "\n",
    "#Train-test Split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Logistic Regression Mode Before Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating regression model\n",
    "logRegBeforeTansf = LogisticRegression(solver='liblinear', random_state=42)\n",
    "# Fit the logistic regression model to the training data\n",
    "logRegBeforeTansf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_predBeforeTansf = logRegBeforeTansf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scores before Data Standardization and Removal of Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for Checking Metrics  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a function for checking for metrics \n",
    "def get_model_metrics(model, X_train, y_train, X_test, y_test):\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the training and testing data\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    roc_auc_train = roc_auc_score(y_train, y_train_pred)\n",
    "    roc_auc_test = roc_auc_score(y_test, y_test_pred)\n",
    "    cm_test = confusion_matrix(y_test, y_test_pred)\n",
    "    cm_display_train = ConfusionMatrixDisplay(confusion_matrix=cm_test).plot()\n",
    "    accuracy_train = accuracy_score(y_train, y_train_pred)\n",
    "    accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "    # Return results\n",
    "    results = {\n",
    "        'ROC-AUC For Train Data': roc_auc_train,\n",
    "        'ROC-AUC For Test Data': roc_auc_test,\n",
    "        'Accuracy Train': accuracy_train,\n",
    "        'Accuracy Test': accuracy_test,\n",
    "        'confusion_matrix_train': cm_display_train\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_model_metrics(logRegBeforeTansf,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The baseline logistic regression exhibits a discrepancy in discrimination levels between the training and testing datasets. The ROC AUC value on the training data is 0.591577190746463, while on the testing data, it stands at 0.5537557289297834. This suggests that the model demonstrates a relatively higher level of discrimination between classes on the training data compared to the testing data.\n",
    "\n",
    "Furthermore, a confusion matrix reveals the predicted and true labels of the logistic regression model. It shows 13 true positives, 88 false negatives, 554 true negatives, and 12 false positives.\n",
    "\n",
    "In summary, the model attains a training accuracy of around 89.4% and a testing accuracy of approximately 85%, indicating competent performance in predicting class labels for both datasets. However, it's worth noting that the model's accuracy in prediction, as depicted by the confusion matrix, is not particularly high, suggesting some degree of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_predBeforeTansf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StandardScaler() will be used to transform features to mean of 0 and standard deviation of 1.\n",
    "This will help the selected features to equally contribute to the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiating standard scaler\n",
    "scaler = StandardScaler()\n",
    "#Scaler fitting on Train data\n",
    "scaler.fit (X_train)\n",
    "\n",
    "#Transform Both Train and Test Data\n",
    "X_train_Scaled = scaler.transform (X_train)\n",
    "X_test_Scaled  = scaler.transform (X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removal of Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE will be used to address underrepresentation of minority class.\n",
    "It generates sythentic samples to represent minority class more accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a instance of SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Perform SMOTE on the training data\n",
    "#X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_resampled.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Logistic Regression Mode After Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating regression model\n",
    "logRegres = LogisticRegression(solver='liblinear', random_state=42)\n",
    "# Fit the logistic regression model to the training data\n",
    "logRegres.fit(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting with the new Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train data prediction\n",
    "y_pred_train = logRegres.predict (X_train_resampled)\n",
    "#Test data predicition\n",
    "y_pred_test  = logRegres.predict (X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scores After  Data Standardization and Removal of Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_model_metrics(logRegres,X_train_resampled,y_train_resampled,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following transformation, the baseline logistic regression has mitigated the gap in discrimination levels between the training and testing datasets. The ROC AUC value is 0.7672942206654991 for the training data and 0.7442448308435083 for the testing data.\n",
    "\n",
    "The confusion matrix displays the logistic regression model's predicted and true labels, indicating 72 true positives, 29 false negatives, 439 true negatives, and 127 false positives. This represents an enhancement over the previous model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Data Accuracy\n",
    "print(classification_report(y_train_resampled, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Data Accuracy\n",
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision: It measures the proportion of correctly identified true positives out of all cases that are predicted as positive (true positives + false positives). In this case, for class 1, the precision is 0.36. This means that out of all the instances predicted as class 1, only 36% were actually correct, indicating a only 36% would actually churn.\n",
    "\n",
    "Recall: It measures the proportion of correctly identified positive cases (true positives) out of all actual positive cases (true positives + false negatives). In this case, for class 1, the recall is 0.71. This means that out of all the actual instances of class 1, 71% were correctly identified by the model, indicating a relatively high recall rate.\n",
    "\n",
    "F1-score:It provides a single score that balances both precision and recall.In this case, for class 1, the F1-score is 0.48, which indicates that the balance between precision and recall for class 1 is moderate.\n",
    "\n",
    "Support: The number of actual occurrences of each class in the test dataset. In this case, there are 566 instances of class 0 and 101 instances of class 1.\n",
    "\n",
    "Accuracy: Overall accuracy of the model, which measures the proportion of correctly classified instances out of the total instances. In this case, the accuracy is 0.77, indicating that the model correctly classified 77% of the instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation ( Perfomance Improvement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of Logistic Regression with cross-validation\n",
    "logreg_cv = LogisticRegressionCV(Cs=10, cv=5, solver='liblinear')\n",
    "\n",
    "# Fit the model on the resampled training data\n",
    "logreg_cv.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predict on the resampled training and testing data\n",
    "y_train_pred_cv = logreg_cv.predict(X_train_resampled)\n",
    "y_test_pred_cv = logreg_cv.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scores After Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_model_metrics(logreg_cv,X_train_resampled,y_train_resampled,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the ROC AUC values are 0.766 for the training data and 0.745 for the testing data. Both values suggest that the model has improved reasonably well in distinguishing between the classes, with slightly higher discrimination observed in the training data compared to the testing data.\n",
    "\n",
    "Confusion matrix shows 72 true positives, 29 false negatives, 440 true negatives, and 29 false positives.This is an improvement compared to the last model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_test_pred_cv, target_names=['0', '1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison Between Cross Validation Model (logreg_cv) and Non-cross Validation Model (logRegres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation Model (logreg_cv)     = Set1 Results\n",
    "# Non-cross Validation Model (logRegres) = Set2 Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison:\n",
    "\n",
    "Precision & Recall: The first set (logreg_cv) of results shows consistent precision and recall values for both classes (0.77 for both), while the second set shows a significant difference between precision and recall for class 1 (0.36 precision, 0.71 recall).\n",
    "\n",
    "F1-score: The F1-score in the first set(logreg_cv) is consistent across both classes (0.77 for both), while in the second set, there's a notable difference between the F1-score for class 0 (0.85) and class 1 (0.48).\n",
    "\n",
    "Conclusion\n",
    "The first (logreg_cv) set of results shows more balanced performance across classes, with consistent precision, recall, and F1-score, while the second set indicates imbalanced performance, particularly for class 1, with much lower precision and F1-score compared to class 0.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drawing ROC curve for the above three models \n",
    "\n",
    "# Compute ROC curves and AUC scores for each model\n",
    "models = [logRegBeforeTansf, logRegres, logreg_cv]\n",
    "labels = ['Model Before Data Transf', 'Model After Data Transf', 'CV Model']\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for model, label in zip(models, labels):\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_probs = model.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        y_probs = model.predict(X_test)\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_probs)\n",
    "    auc_score = roc_auc_score(y_test, y_probs)\n",
    "\n",
    "    plt.plot(fpr, tpr, label='{} (AUC = {:.2f})'.format(label, auc_score))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curves')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation, Training, and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object creation, fitting the data & getting predictions\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_model_final = RandomForestClassifier() \n",
    "rf_model_final.fit(X_train_resampled,y_train_resampled) \n",
    "y_pred_rf = rf_model_final.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Importance =pd.DataFrame({\"Importance\": rf_model_final.feature_importances_*100},index = X_train_resampled.columns)\n",
    "Importance.sort_values(by = \"Importance\", axis = 0, ascending = True).tail(15).plot(kind =\"barh\", color = \"r\",figsize=(9, 5))\n",
    "plt.title(\"Feature Importance Levels\");\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_rf, target_names=['0', '1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Performance Summary Before Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"**************** RANDOM FOREST MODEL RESULTS **************** \")\n",
    "print('Accuracy score for testing set: ',round(accuracy_score(y_test,y_pred_rf),5))\n",
    "print('F1 score for testing set: ',round(f1_score(y_test,y_pred_rf),5))\n",
    "print('Recall score for testing set: ',round(recall_score(y_test,y_pred_rf),5))\n",
    "print('Precision score for testing set: ',round(precision_score(y_test,y_pred_rf),5))\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "f, ax= plt.subplots(1,1,figsize=(5,3))\n",
    "sns.heatmap(cm_rf, annot=True, cmap='Reds', fmt='g', ax=ax)\n",
    "ax.set_xlabel('Predicted Labels'); ax.set_ylabel('True Labels') ; ax.set_title('Confusion Matrix')\n",
    "ax.xaxis.set_ticklabels(['0', '1']) ; ax.yaxis.set_ticklabels(['0', '1'])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random forest classifier object\n",
    "rf_model = RandomForestClassifier()\n",
    "\n",
    "# Define the scoring metrics for cross-validation\n",
    "scoring_metrics = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'precision': make_scorer(precision_score, zero_division=0),\n",
    "    'recall': make_scorer(recall_score, zero_division=0),\n",
    "    'f1_score': make_scorer(f1_score, zero_division=0)\n",
    "}\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "cv_results = cross_validate(rf_model, X_train_resampled, y_train_resampled, cv=5, scoring=scoring_metrics)\n",
    "\n",
    "# Convert the cross-validation results to a readable format\n",
    "cv_results_readable = {metric: np.mean(scores) for metric, scores in cv_results.items() if metric.startswith('test_')}\n",
    "\n",
    "# Output the results\n",
    "for metric, score in cv_results_readable.items():\n",
    "    print(f\"{metric}: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Performance Summary After Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_model_metrics(rf_model,X_train_resampled,y_train_resampled,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation, Training, and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Object creation, fitting the data & getting predictions\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "decision_tree.fit(X_train_resampled,y_train_resampled)\n",
    "y_pred_dt = decision_tree.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = list(X_train_resampled.columns)\n",
    "importances = decision_tree.feature_importances_[0:15]\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='green', align='center')\n",
    "plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(classification_report(y_test, y_pred_dt, target_names=['0', '1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance Summary Before Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"**************** DECISION TREE CLASSIFIER MODEL RESULTS **************** \")\n",
    "print('Accuracy score for testing set: ',round(accuracy_score(y_test,y_pred_dt),5))\n",
    "print('F1 score for testing set: ',round(f1_score(y_test,y_pred_dt),5))\n",
    "print('Recall score for testing set: ',round(recall_score(y_test,y_pred_dt),5))\n",
    "print('Precision score for testing set: ',round(precision_score(y_test,y_pred_dt),5))\n",
    "cm_dt = confusion_matrix(y_test, y_pred_dt)\n",
    "f, ax= plt.subplots(1,1,figsize=(5,3))\n",
    "sns.heatmap(cm_dt, annot=True, cmap='Greens', fmt='g', ax=ax)\n",
    "ax.set_xlabel('Predicted Labels'); ax.set_ylabel('True Labels') ; ax.set_title('Confusion Matrix')\n",
    "ax.xaxis.set_ticklabels(['0', '1']) ; ax.yaxis.set_ticklabels(['0', '1'])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Create a decision tree classifier object\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "# Define the scoring metrics for cross-validation\n",
    "scoring_metrics = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'precision': make_scorer(precision_score, zero_division=0),\n",
    "    'recall': make_scorer(recall_score, zero_division=0),\n",
    "    'f1_score': make_scorer(f1_score, zero_division=0)\n",
    "}\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "cv_results = cross_validate(decision_tree, X_train_resampled, y_train_resampled, cv=5, scoring=scoring_metrics)\n",
    "\n",
    "# Convert the cross-validation results to a readable format\n",
    "cv_results_readable = {metric: np.mean(scores) for metric, scores in cv_results.items() if metric.startswith('test_')}\n",
    "\n",
    "# Output the results\n",
    "for metric, score in cv_results_readable.items():\n",
    "    print(f\"{metric}: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance Summary After Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_model_metrics(decision_tree,X_train_resampled,y_train_resampled,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perfomance Summary of the Three Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest classifier demonstrates strong performance with an accuracy of approximately 95.95% on the testing data. It effectively distinguishes between positive and negative classes, boasting an area under the ROC curve (AUC) of 1.0 on the training data and 0.8576863870132596 on the testing data. Overall, the model exhibits robust predictive capabilities, yielding a high level of accuracy in predicting the target variable.\n",
    "\n",
    "The confusion matrix reveals 76 true positives (TP), 545 true negatives (TN), 21 false positives (FP), and 25 false negatives (FN).\n",
    "\n",
    "Comparing the three models, it's evident that logistic regression underperforms in predicting customer churn. In contrast, both the Random Forest classifier and Decision Trees exhibit strong performance, achieving accuracies of 95.95% and 88.0%, respectively.\n",
    "\n",
    "Given the superior predictability of the Random Forest classifier and Decision Trees, it's pertinent to enhance these models further using hyperparameters to optimize accuracy. Hyperparameters serve as a valuable tool for improving efficiency and performance across models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree and Random forester performs relatively better than logistic regression therefore,we can drop the latter and hypertune the other two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This the process of selecting optimal hyperparameters fo our models.They are set before the model begins to learn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest HyperTuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "# Create the Random Forest classifier\n",
    "rf_clf = RandomForestClassifier()\n",
    "\n",
    "# Instantiate the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=rf_clf, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit the GridSearchCV object to the data\n",
    "grid_search.fit(X_train_resampled,y_train_resampled)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Create a new Random Forest classifier with the best hyperparameters\n",
    "best_RForest_model = RandomForestClassifier(**best_params, random_state=42)\n",
    "\n",
    "# Fit the best model to the resampled training data\n",
    "best_RForest_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predict on the training data\n",
    "y_train_pred = best_model.predict(X_train_resampled)\n",
    "\n",
    "# Predict on the test data\n",
    "y_test_pred = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perfomance Summary AfterAdding Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_model_metrics(best_RForest_model,X_train_resampled,y_train_resampled,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision HyperTuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Define the hyperparameters grid\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Create a Decision Tree classifier\n",
    "dt_classifier = DecisionTreeClassifier()\n",
    "\n",
    "# Instantiate GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=dt_classifier, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the GridSearchCV object to the data\n",
    "grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Instantiate the DecisionTreeClassifier with the best hyperparameters\n",
    "best_decTree_classifier = DecisionTreeClassifier(**best_params)\n",
    "\n",
    "# Fit the model with the best hyperparameters\n",
    "best_decTree_classifier.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perfomance Summary After Adding Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "get_model_metrics(best_decTree_classifier,X_train_resampled,y_train_resampled,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curves and AUC scores for each model\n",
    "models = [ logreg_cv,best_decTree_classifier,best_RForest_model]\n",
    "labels = ['logistic Model','Decision Tree',' Random Fores']\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for model, label in zip(models, labels):\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_probs = model.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        y_probs = model.predict(X_test)\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_probs)\n",
    "    auc_score = roc_auc_score(y_test, y_probs)\n",
    "\n",
    "    plt.plot(fpr, tpr, label='{} (AUC = {:.2f})'.format(label, auc_score))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curves')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
